# FastAPI Server Configuration
FastAPI:
    HOST: "0.0.0.0"
    PORT: "8000"
    WORKERS: "2"
    LOG_LEVEL: "info"
    TIMEOUT: "240"
    GRACEFUL_TIMEOUT: "60"


# MongoDB Configuration
MongoDB:
    MONGO_URL: "mongodb://localhost:27017"
    DB_NAME: "ai_chat_app"
    USER_COLLECTION: "users"
    CHAT_HISTORY_COLLECTION: "chats"
    MESSAGES_COLLECTION: "chat_messages"
    

# Security Configuration
Security:
    ALGORITHM: "HS256"
    ACCESS_TOKEN_EXPIRE_MINUTES: 360 # 6 hours


# Email Configuration
Email:
    SMTP_SERVER: "smtp.gmail.com"
    SMTP_PORT: 587
    

# Logging Configuration
Logging:
    LOG_DIR: "logs"
    LOG_LEVEL: "info"
    DEBUG_LOG_FILE_NAME: "aichatapp-debug.log"
    INFO_LOG_FILE_NAME: "aichatapp-info.log"
    WARNING_LOG_FILE_NAME: "aichatapp-warning.log"
    ERROR_LOG_FILE_NAME: "aichatapp-error.log"
    MAX_FILE_SIZE: 5242880   # 5 MB in bytes
    MAX_FILE_COUNT: 10       # 10 files
    LOG_FORMAT: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"


# Services Configuration
Services:
    SUPPORTED_SERVICES: ["chat", "web_search"]
    SUPPORTED_MODEL_TYPE: ["instruct", "chat", "reasoning"]
    SUPPORTED_LLM_PROVIDER: ["ollama", "vllm", "aws_bedrock", "groq", "nvidia", "openai", "llamacpp", "google", "huggingface"]


# LLM Configuration
LLM:
    Provider: "ollama"
    ollama:
        BASE_URL: "http://0.0.0.0:11434"
        MODEL: "nemotron-3-nano:30b-cloud"
        MAX_TOKENS: 4096
        TEMPERATURE: 0.5
        TOP_P: 1.0
        FREQUENCY_PENALTY: 0.0
        PRESENCE_PENALTY: 0.0
        REASONING_EFFORT: "low"
        MODEL_TYPE: "instruct"

    vllm:                                                
        API_KEY: "EMPTY"
        BASE_URL: "http://10.90.12.105:11434/v1"
        MODEL: "openai.gpt-oss-20b-1:0"
        MAX_TOKENS: 4096
        TEMPERATURE: 0.5
        TOP_P: 1.0
        FREQUENCY_PENALTY: 0.0
        PRESENCE_PENALTY: 0.0
        REASONING_EFFORT: "low"
        MODEL_TYPE: "reasoning"

    aws_bedrock:
        AWS_REGION: "ap-south-1"
        MODEL: "openai.gpt-oss-20b-1:0"
        MAX_TOKENS: 4096
        TEMPERATURE: 0.5
        TOP_P: 1.0
        FREQUENCY_PENALTY: 0.0
        PRESENCE_PENALTY: 0.0
        REASONING_EFFORT: "low"
        MODEL_TYPE: "reasoning" 

    groq:
        MODEL: "meta-llama/llama-4-scout-17b-16e-instruct"
        MAX_TOKENS: 4096
        TEMPERATURE: 0.5
        REASONING_EFFORT: "low"
        MODEL_TYPE: "instruct"

    nvidia:
        MODEL: "nvidia/nemotron-nano-12b-v2-vl"
        MAX_TOKENS: 4096
        TEMPERATURE: 0.5
        REASONING_EFFORT: "low"
        MODEL_TYPE: "instruct"

    google:
        MODEL: "gemini-2.0-flash"
        MAX_TOKENS: 4096
        TEMPERATURE: 0.5
        REASONING_EFFORT: "low"
        MAX_RETRIES: 2
        MODEL_TYPE: "instruct"

    huggingface:
        MODEL: "openai/gpt-oss-20b"
        MAX_TOKENS: 4096
        TEMPERATURE: 0.5
        REASONING_EFFORT: "low"
        MODEL_TYPE: "text-generation"
        STREAMING: False
        PROVIDER: "auto" # let Hugging Face choose the best provider for you

    llamacpp:
        MODEL: "reports/qwen2.5-1.5b-instruct-q4_k_m.gguf" # Path to the local model file
        MAX_TOKENS: 4096       # Maximum number of tokens to generate
        TEMPERATURE: 0.5       # controls how random the choice is; lower = more deterministic, higher = more creative
        N_CTX: 512             # Number of tokens to keep in the context
        N_GPU_LAYERS: -1       # -1 = all layers on GPU
        N_BATCH: 300           # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
        REPEAT_PENALTY: 1.5    # Reduces repeated words; higher = less repetition, 1.0 = no penalty, 1.2 = 20% penalty
        TOP_P: 0.5             # Controls creativity; lower = more focused responses (controls how many choices are allowed)
        VERBOSE: True          # Enable verbose output

